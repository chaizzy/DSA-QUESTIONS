{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75659b45-31c4-49b3-b82d-1478ea519659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANS 1:\n",
    "# 1.Data Ingestion Pipeline:\n",
    "\n",
    "# a : Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.\n",
    "# To design a data ingestion pipeline in Python that collects and stores data from various sources,\n",
    "# such as databases, APIs, and streaming platforms, you can follow these general steps:\n",
    "\n",
    "# 1. Identify Data Sources:\n",
    "# Determine the data sources you want to collect from, such as databases, APIs, or streaming platforms.\n",
    "# Understand the data formats, access methods, and any authentication or authorization requirements for each source.\n",
    "\n",
    "# 2. Choose Data Collection Tools and Libraries:\n",
    "# Select appropriate Python libraries and tools that can interact with the different data sources.\n",
    "# For databases, you can use libraries like SQLAlchemy, psycopg2, or pymongo, depending on the database type.\n",
    "# For APIs, libraries like requests or specialized API wrappers can be used.\n",
    "# For streaming platforms, libraries like Kafka-Python, Pulsar, or Apache Beam can be utilized.\n",
    "\n",
    "\n",
    "# 3.Establish Connection and Authentication:\n",
    "# Set up connections to the data sources using appropriate connection parameters or credentials.\n",
    "# Configure authentication methods, such as API keys or OAuth tokens, as required by the data sources.\n",
    "\n",
    "# 4.Fetch and Collect Data:\n",
    "# Write functions or classes to fetch data from each source, using the corresponding libraries.\n",
    "# For databases, you can write SQL queries or use ORM (Object-Relational Mapping) techniques.\n",
    "# For APIs, make HTTP requests and process the responses to extract the required data.\n",
    "# For streaming platforms, set up consumer or subscriber clients to consume the data stream.\n",
    "\n",
    "\n",
    "# 5.Handle Data Transformations and Preprocessing:\n",
    "# Perform any necessary data transformations or preprocessing steps to clean, format, or enrich the collected data.\n",
    "# Use appropriate libraries for data manipulation, cleaning, and transformation, such as pandas or NumPy.\n",
    "\n",
    "# 6 .Define Storage Mechanisms:\n",
    "# Determine the storage mechanisms based on your requirements, such as databases, data lakes, or file systems.\n",
    "# Choose suitable storage technologies like PostgreSQL, MySQL, MongoDB, Apache Hadoop, Apache Parquet, or Amazon S3.\n",
    "\n",
    "# 7.Write Data to Storage:\n",
    "# Develop code to write the collected and processed data to the chosen storage mechanisms.\n",
    "# Utilize appropriate libraries or database connectors to insert or write the data.\n",
    "# Ensure data integrity, consistency, and error handling during the writing process\n",
    "\n",
    "# 8 .Implement Scheduling and Automation:\n",
    "# Set up scheduling mechanisms, such as cron jobs or task schedulers, to automate the data ingestion pipeline.\n",
    "# Determine the frequency of data collection and define the intervals or triggers accordingly.\n",
    "\n",
    "# 9.Implement Error Handling and Logging:\n",
    "# Include error handling mechanisms to handle exceptions or failures during data collection or storage.\n",
    "# Use logging frameworks, such as Python's built-in logging module or third-party libraries like loguru or structlog, to log pipeline activities, errors, and information.\n",
    "\n",
    "# 10.Monitor and Maintain:\n",
    "# Monitor the data ingestion pipeline for performance, data quality, and any potential issues.\n",
    "# Implement monitoring and alerting mechanisms to identify and address any pipeline failures or anomalies.\n",
    "# Regularly review and maintain the pipeline to adapt to changes in data sources or requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c2fac8-237e-4226-8565-ac7bcb512dbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "# example\n",
    "import requests\n",
    "import json\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pymongo\n",
    "from kafka import KafkaConsumer\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the database\n",
    "db_conn = psycopg2.connect(database=\"your_database\", user=\"your_username\", password=\"your_password\", host=\"localhost\", port=\"5432\")\n",
    "db_cursor = db_conn.cursor()\n",
    "\n",
    "# Connect to MongoDB\n",
    "mongo_client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "mongo_db = mongo_client[\"your_database\"]\n",
    "mongo_collection = mongo_db[\"your_collection\"]\n",
    "\n",
    "# Create an engine for SQLAlchemy\n",
    "db_engine = create_engine('postgresql://your_username:your_password@localhost:5432/your_database')\n",
    "\n",
    "# Connect to Kafka\n",
    "consumer = KafkaConsumer('your_topic', bootstrap_servers=['localhost:9092'])\n",
    "# Fetch data from API\n",
    "response = requests.get('your_api_url')\n",
    "api_data = json.loads(response.text)\n",
    "\n",
    "# Process and store the API data\n",
    "processed_api_data = process_api_data(api_data)\n",
    "db_cursor.execute(\"INSERT INTO your_table (column1, column2) VALUES (%s, %s)\", (processed_api_data['value1'], processed_api_data['value2']))\n",
    "db_conn.commit()\n",
    "\n",
    "# Fetch and process data from Kafka\n",
    "for message in consumer:\n",
    "    kafka_data = json.loads(message.value)\n",
    "    processed_kafka_data = process_kafka_data(kafka_data)\n",
    "    mongo_collection.insert_one(processed_kafka_data)\n",
    "\n",
    "# Fetch data from a database table\n",
    "query = \"SELECT * FROM your_table\"\n",
    "df = pd.read_sql_query(query, db_engine)\n",
    "\n",
    "# Perform data transformations and preprocessing\n",
    "transformed_data = transform_data(df)\n",
    "\n",
    "# Store the transformed data in a file\n",
    "transformed_data.to_csv('transformed_data.csv', index=False)\n",
    "# Close database connections and Kafka consumer\n",
    "db_cursor.close()\n",
    "db_conn.close()\n",
    "mongo_client.close()\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284b6a64-3b42-430a-9884-ed9d1a240080",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paho'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# To implement a real-time data ingestion pipeline for processing sensor data from IoT devices in Python, \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# you can utilize a combination of technologies such as MQTT (Message Queuing Telemetry Transport) \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# protocol, MQTT broker, and a Python MQTT client library. Here's an example code snippet that demonstrates the basic implementation of such a pipeline:\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpaho\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmqtt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmqtt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'paho'"
     ]
    }
   ],
   "source": [
    "# b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.\n",
    "# To implement a real-time data ingestion pipeline for processing sensor data from IoT devices in Python, \n",
    "# you can utilize a combination of technologies such as MQTT (Message Queuing Telemetry Transport) \n",
    "# protocol, MQTT broker, and a Python MQTT client library. Here's an example code snippet that demonstrates the basic implementation of such a pipeline:\n",
    "\n",
    "import paho.mqtt.client as mqtt\n",
    "import json\n",
    "import time\n",
    "\n",
    "# MQTT broker settings\n",
    "broker_address = \"mqtt_broker_address\"\n",
    "broker_port = 1883\n",
    "topic = \"your_topic\"\n",
    "\n",
    "# Define callback functions for MQTT events\n",
    "def on_connect(client, userdata, flags, rc):\n",
    "    print(\"Connected to MQTT broker with result code: \" + str(rc))\n",
    "    client.subscribe(topic)\n",
    "\n",
    "def on_message(client, userdata, msg):\n",
    "    payload = msg.payload.decode(\"utf-8\")\n",
    "    data = json.loads(payload)\n",
    "\n",
    "    # Process and analyze the received sensor data\n",
    "    process_sensor_data(data)\n",
    "# Create an MQTT client instance\n",
    "client = mqtt.Client()\n",
    "\n",
    "# Set MQTT event callbacks\n",
    "client.on_connect = on_connect\n",
    "client.on_message = on_message\n",
    "\n",
    "# Connect to the MQTT broker\n",
    "client.connect(broker_address, broker_port, 60)\n",
    "\n",
    "# Start the MQTT client loop to handle incoming messages\n",
    "client.loop_start()\n",
    "\n",
    "# Continuously process sensor data until interrupted\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted, stopping data ingestion.\")\n",
    "\n",
    "# Disconnect from the MQTT broker\n",
    "client.disconnect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf9488-ee4e-42c9-b356-62fb64fc414e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
